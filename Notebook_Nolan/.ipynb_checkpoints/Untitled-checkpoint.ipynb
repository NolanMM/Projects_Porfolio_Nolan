{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762ca76b-a49b-4f31-a8ba-1329dd39ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier \n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b10c89-b5ab-423c-a2cd-02db9bdf04f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sparkify_Project_Spark_Local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_path = \"./mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(data_path)\n",
    "# check the schema of the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edfa059-e6d1-487f-8b02-ddb1a4b06db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 286500 rows.\n"
     ]
    }
   ],
   "source": [
    "print('The dataset has {} rows.'.format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716dc419-84db-4044-91c0-ac0a1c0d6ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         Start time|\n",
      "+-------------------+\n",
      "|2018-09-30 20:01:57|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(to_timestamp(col('ts')/1000)).alias('Start time')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71da415a-7721-4515-be4b-6a3b67971332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           End time|\n",
      "+-------------------+\n",
      "|2018-12-02 20:11:16|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(to_timestamp(col('ts')/1000)).alias('End time')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d0c01d-7dc7-4b77-afaa-d643a2d568cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: bigint, auth: bigint, firstName: bigint, gender: bigint, itemInSession: bigint, lastName: bigint, length: bigint, level: bigint, location: bigint, method: bigint, page: bigint, registration: bigint, sessionId: bigint, song: bigint, status: bigint, ts: bigint, userAgent: bigint, userId: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.select([count(when(isnan(c),c)).alias(c) for c in df.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28201cb7-f7bb-41c3-bea6-baad1aed8da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"page\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f647804a-a6ca-4b67-ad61-8e71a81be846",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancellation_check_function = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "# Applied and generated new columns named churn\n",
    "df = df.withColumn(\"churn\", cancellation_check_function(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30bfc28-4113-476f-aa23-126813150488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum as Fsum\n",
    "\n",
    "# Define the window bounds to use Fsum to count for the churn\n",
    "windowval = Window.partitionBy(\"userId\").rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "# Apply the window function to the DataFrame df\n",
    "df = df.withColumn(\"churn\", Fsum(col(\"churn\")).over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3273656b-2051-4b24-a514-da8fd66c1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Missing values]\n",
      "\n",
      "artist: 58392\n",
      "firstName: 8346\n",
      "gender: 8346\n",
      "lastName: 8346\n",
      "length: 58392\n",
      "location: 8346\n",
      "registration: 8346\n",
      "song: 58392\n",
      "userAgent: 8346\n",
      "userId: 8346\n"
     ]
    }
   ],
   "source": [
    "def missing_values(df, col):\n",
    "    return df.filter((isnan(df[col])) | (df[col].isNull()) | (df[col] == \"\")).count()\n",
    "\n",
    "print(\"\\n[Missing values]\\n\")\n",
    "for col in df.columns:\n",
    "    missing_count = missing_values(df, col)\n",
    "    if missing_count > 0:\n",
    "        print(\"{}: {}\".format(col, missing_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62e03c23-ce7b-42b1-bdfd-e7b9ca991d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(df[\"userId\"] != \"\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f9e990-ffda-43f4-9b65-1dca9b432255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg churn rate of females is: 19.230769230769234\n",
      "The avg churn rate of males is: 26.446280991735538\n"
     ]
    }
   ],
   "source": [
    "stat_df = spark.createDataFrame(df.dropDuplicates(['userId']).collect())\n",
    "stat_df1 = stat_df[['gender', 'churn']]\n",
    "print('The avg churn rate of females is:', stat_df1.groupby(['gender']).mean().collect()[0][1]*100)\n",
    "print('The avg churn rate of males is:', stat_df1.groupby(['gender']).mean().collect()[1][1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e9f555-0569-4513-a3fe-02579a3212d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, sum(churn)=27),\n",
       " Row(artist='P!nk', sum(churn)=1),\n",
       " Row(artist='Gorillaz', sum(churn)=1),\n",
       " Row(artist='Modjo', sum(churn)=1),\n",
       " Row(artist=\"Christopher O'Riley\", sum(churn)=1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stat_df1 = stat_df[['artist', 'churn']]\n",
    "display(stat_df1.groupBy(['artist']).sum().orderBy('sum(churn)', ascending = False).collect()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d437566-ce5f-4cb8-84d7-b6f4adfe9a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
